

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Pondering upon Eigenvectors - Piyush Tiwary</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Piyush Tiwary">
<meta property="og:title" content="Pondering upon Eigenvectors">


  <link rel="canonical" href="https://backpropagator.github.io/posts/eigenvector.md/">
  <meta property="og:url" content="https://backpropagator.github.io/posts/eigenvector.md/">



  <meta property="og:description" content="The Eigenvectors &amp; Eigenvalues pop up in many places of mathematical analysis and application. For example: Machine Learning, Control Theory, Signal Processing, Quantum Physics, Markov Process just to name a few! Despite it’s wide applications, it isn’t entirely clear as to what do these Eigenvectors represent and what is their significance in all these applications. Also, many students see Eigenvectors as a bunch of steps to find a vector which is going to help them, and hence they have very little intuition about them. In this post, I plan to give an intuition of the same and also provide some insights of how it is calculated algorithmically.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2020-06-17T00:00:00-07:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Piyush Tiwary",
      "url" : "https://backpropagator.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://backpropagator.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Piyush Tiwary Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://backpropagator.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://backpropagator.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://backpropagator.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://backpropagator.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://backpropagator.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://backpropagator.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://backpropagator.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://backpropagator.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://backpropagator.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://backpropagator.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://backpropagator.github.io/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="https://backpropagator.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://backpropagator.github.io/images/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="https://backpropagator.github.io/images/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="https://backpropagator.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://backpropagator.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://backpropagator.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://backpropagator.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://backpropagator.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://backpropagator.github.io/">Piyush Tiwary</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://backpropagator.github.io/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://backpropagator.github.io/talks/">Talks</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://backpropagator.github.io/projects/">Projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://backpropagator.github.io/blog-post/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://backpropagator.github.io/cv/">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://backpropagator.github.io/contact/">Contact</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://backpropagator.github.io/images/profile.png" class="author__avatar" alt="Piyush Tiwary">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Piyush Tiwary</h3>
    <p class="author__bio">Ph.D. Scholar at IISc Bangalore</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Bangalore, India</li>
      
      
      
      
      
       
      
        <li><a href="https://twitter.com/backpropogator"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
      
      
        <li><a href="https://www.linkedin.com/in/thebackpropogator"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
        <li><a href="https://instagram.com/thebackpropogator"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
      
      
      
      
        <li><a href="https://github.com/backpropagator"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=tUdHYloAAAAJ&hl=en"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Pondering upon Eigenvectors">
    <meta itemprop="description" content="The Eigenvectors &amp; Eigenvalues pop up in many places of mathematical analysis and application. For example: Machine Learning, Control Theory, Signal Processing, Quantum Physics, Markov Process just to name a few! Despite it’s wide applications, it isn’t entirely clear as to what do these Eigenvectors represent and what is their significance in all these applications. Also, many students see Eigenvectors as a bunch of steps to find a vector which is going to help them, and hence they have very little intuition about them. In this post, I plan to give an intuition of the same and also provide some insights of how it is calculated algorithmically.">
    <meta itemprop="datePublished" content="June 17, 2020">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Pondering upon Eigenvectors
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  9 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-06-17T00:00:00-07:00">June 17, 2020</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>The Eigenvectors &amp; Eigenvalues pop up in many places of mathematical analysis and application. For example: Machine Learning, Control Theory, Signal Processing, Quantum Physics, Markov Process just to name a few! Despite it’s wide applications, it isn’t entirely clear as to what do these Eigenvectors represent and what is their significance in all these applications. Also, many students see Eigenvectors as a bunch of steps to find a vector which is going to help them, and hence they have very little intuition about them. In this post, I plan to give an intuition of the same and also provide some insights of how it is calculated algorithmically.</p>

<h2 id="matrix-as-transformations">Matrix as Transformations!</h2>

<p>Well to understand this, we need to understand what a <strong>“Matrix”</strong> is? Generally, matrices are seen as a bunch of numbers inside a box used to perform collective operations. It’s one way to look at it. But a higher level insight is to look at matrices as a <strong>Transformation</strong>!</p>

<p>Let’s take an example, let <script type="math/tex">% <![CDATA[
A = \left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array} \right) %]]></script>.
If we multiply any vector
<script type="math/tex">\alpha = \left( \begin{array}{c} \alpha_1 \\ \alpha_2 \\ \end{array} \right)</script> with <script type="math/tex">A</script>, we’ll get the same vector. Not interesting right?</p>

<p>But here is where it gets interesting, we can look at this multiplication in 2 ways. One way is</p>

<p>$A\alpha = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} \alpha_1 \\ \alpha_2 \end{pmatrix} + 
\begin{pmatrix} 0 &amp; 1 \end{pmatrix} \begin{pmatrix} \alpha_1 \\ \alpha_2 \end{pmatrix}$</p>

<p>or,</p>

<script type="math/tex; mode=display">A\alpha = \left( \begin{array}{cc} (1*\alpha_1) + (0*\alpha_2) \\ (0*\alpha_1) + (1*\alpha_2) \\ \end{array} \right)</script>

<p>which is the traditional way to multiply 2 matrices. But there’s another way to look at it, instead of multiplying rows of by columns, we can multiply columns by rows, here’s what I mean -</p>

<script type="math/tex; mode=display">A\alpha = \left( \begin{array}{c} 1 \\ 0 \\ \end{array} \right)\alpha_1 + \left( \begin{array}{c} 0 \\ 1 \\ \end{array} \right)\alpha_2</script>

<p>from this way of multiplication, we can see that this multiplication is essentially addition of two seperate column vectors, where each column vector is obtained by taking the columns of <script type="math/tex">A</script> and <strong>scaling</strong> them by elements of <script type="math/tex">\alpha</script>! In other words, if we want to find the product of a matrix with a vector, just take the columns of the matrix, scale them by corresponding amount in the vector and sum them up. So, we scale the first column by <script type="math/tex">\alpha_1</script> and second column by <script type="math/tex">\alpha_2</script> and add them up.</p>

<p>Does this seem familiar? If you were thinking of Vectors, you are right! This is indeed the essence of a Matrix that <strong>the columns of a Matrix represent the vectors which span the range of the Matrix</strong>, these vectors are also called <strong>Basis Vectors</strong>.</p>

<p>Going back to our example, the columns of <script type="math/tex">A</script> :
<script type="math/tex">\left( \begin{array}{c} 1 \\ 0 \\ \end{array} \right)</script> 
&amp;
<script type="math/tex">\left( \begin{array}{c} 0 \\ 1 \\ \end{array} \right)</script>
in a way, represent the directions in which we need to travel to get our answer (the product), so you need to go <script type="math/tex">\alpha_1</script> amount in the direction of
<script type="math/tex">\left( \begin{array}{c} 1 \\ 0 \\ \end{array} \right)</script> 
and <script type="math/tex">\alpha_2</script> amount in the direction of
<script type="math/tex">\left( \begin{array}{c} 0 \\ 1 \\ \end{array} \right)</script> 
to get the product <script type="math/tex">A\alpha</script>.<br />
In terms of vectors, one can interpret the columns of matrix as vector directions of <script type="math/tex">\hat i</script> &amp; <script type="math/tex">\hat j</script> and the multiplication <script type="math/tex">A\alpha</script> is nothing but the vector <script type="math/tex">\alpha_1 \hat i + \alpha_2 \hat j</script>.</p>

<p>Cool! but what if the matrix isn’t trivial like this one? say 
<script type="math/tex">% <![CDATA[
A = \left( \begin{array}{cc} x_1 & x_2 \\ y_1 & y_2 \\ \end{array} \right) %]]></script>
this makes a very little difference. Instead of the component directions being <script type="math/tex">\hat i</script> &amp; <script type="math/tex">\hat j</script>, the direction of each column will be <script type="math/tex">(x_1 \hat i + y_1 \hat j)</script> &amp; <script type="math/tex">(x_2 \hat i + y_2 \hat j)</script> respectively.
And the product <script type="math/tex">A \alpha</script> is nothing but  <script type="math/tex">\alpha_1 (x_1 \hat i + y_1 \hat j)  + \alpha_2 (x_2 \hat i + y_2 \hat j)</script>! This is really a nice way to look at matrix multiplications which relate Matrices to Vectors in such an intuitive way!</p>

<p>Now, where is transformation in all this? For this we need to look at <script type="math/tex">\alpha</script>. <br />
Any <script type="math/tex">\alpha</script> with dimension <script type="math/tex">(2 \times 1)</script> can represent a point in 2-dimension.  So, <script type="math/tex">\alpha \in \mathcal{R}^2</script> and after multiplying by <script type="math/tex">A</script>, the product <script type="math/tex">A\alpha \in \mathcal{R}^2</script> too. So, we can say that <script type="math/tex">A</script> maps a vector from <script type="math/tex">\mathcal{R}^2</script> to <script type="math/tex">\mathcal{R}^2</script>, <strong>but is it the same Coordinate System?</strong> The answer is <strong>No!</strong> the coordinate system has changed indeed but how? Think of it this way - originally, <script type="math/tex">\alpha</script> was in original <script type="math/tex">2D</script> plane whose axis were 
<script type="math/tex">\left( \begin{array}{c} 1 \\ 0 \\ \end{array} \right)</script> 
&amp;
<script type="math/tex">\left( \begin{array}{c} 0 \\ 1 \\ \end{array} \right)</script> 
(or <script type="math/tex">\hat i</script> &amp; <script type="math/tex">\hat j</script>), so to go to any point we need to go some distance along <script type="math/tex">\hat i</script> and some distance along <script type="math/tex">\hat j</script>, but after multiplying it by <script type="math/tex">A</script>, the axis are changed to 
<script type="math/tex">\left( \begin{array}{c} x_1 \\ y_1 \\ \end{array} \right)</script> 
&amp;
<script type="math/tex">\left( \begin{array}{c} x_2 \\ y_2 \\ \end{array} \right)</script> 
(or <script type="math/tex">(x_1 \hat i + y_1 \hat j)</script> &amp; <script type="math/tex">(x_2 \hat i + y_2 \hat j)</script>). So, can you see the transformation? The axis of the coordinate system is changed after multiplying with a matrix. More clearly-</p>

<p><script type="math/tex">\left( \begin{array}{c} 1 \\ 0 \\ \end{array} \right) \rightarrow \left( \begin{array}{c} x_1 \\ y_1 \\ \end{array} \right)</script>
   &amp;   
<script type="math/tex">\left( \begin{array}{c} 0 \\ 1 \\ \end{array} \right) \rightarrow \left( \begin{array}{c} x_2 \\ y_2 \\ \end{array} \right)</script> <br />
In literature this axis is also called <strong>Basis Vector</strong>. Basis of a Matrix is a set of vector through which we can get any point (in that dimension) by taking some linear combination of these vectors. There’s more to this, but I leave it to the reader to explore for themselves.</p>

<h2 id="what-are-these-eigen-things">What are these “Eigen” things?</h2>

<p>Having an intuition of how matrix can be seen as a Transformation, we are in a position to understand the Eigenvectors &amp; Eigenvalues.</p>

<p>So, while transforming a point from one basis to another by multiplication of a matrix, we map a point original 2D plane to some other point in the transformed plane. If we look at the vectors representing these points, they are also being transformed from one coordinate system to another. However, <strong>there are some vectors that don’t change their direction, instead they only get scaled up or scaled down!</strong> These vectors are nothing but the Eigenvectors! and the amount by which they get scaled up or down is the corresponding Eigenvalue!</p>

<p>Mathematically, these are the vectors that after multiplying by <script type="math/tex">A</script> get scaled up or down but the direction remains the same. For equation form we need, <script type="math/tex">Ax</script> which is in the same direction as <script type="math/tex">x</script> but scaled up or down (let’s say by <script type="math/tex">\lambda</script>). So,</p>

<script type="math/tex; mode=display">Ax = \lambda x</script>

<p>Which is the equation we have been taught since childhood to calculate the eigenvectors &amp; eigenvalues. From this equation, we can find the Eigenvalues of A by solving</p>

<script type="math/tex; mode=display">|A - \lambda\mathbb{I}| = 0</script>

<p>, to find the eigenvectors, we will have to find the Basis of Null-Space of
<script type="math/tex">A - \lambda\mathbb{I}</script>
. Null Space of a matrix <script type="math/tex">A</script> (also written as <script type="math/tex">N(A)</script>) contains all the vectors <script type="math/tex">x</script> such that, <script type="math/tex">Ax=0</script>, so finding basis of 
<script type="math/tex">N(A-\lambda\mathbb{I})</script> 
means that we have to find basis of all the vectors for which <script type="math/tex">(A - \lambda\mathbb{I})x = 0</script> which is the definition of Eigenvectors.</p>

<h2 id="some-things-to-look-for">Some things to look for</h2>

<p>Okay! so upto this point we have understood what is the physical significance of Eigenvectors. Now there are somethings, we should be aware of. Let’s first define some terms.</p>

<p><strong>Span</strong>: The <strong>span</strong> of a set of vectors is the set of all Linear Combinations of the vectors. Implicitly, Span indicates the dimension that can be covered by using a set of vectors.</p>

<p><strong>Rank</strong>: Rank of a matrix is defined as the maximum number of linearly independent columns in a matrix. This is the textbook defiition of Rank. It’s physical significance is that, Rank of a matrix denotes the dimension of the transformed coordinate system. To understand this, let’s take an example 
<script type="math/tex">% <![CDATA[
A = \left( \begin{array}{ccc} 1 & 2 & 5 \\ 3 & 5 & 13 \\ 7 & 6 & 19 \\ \end{array} \right) %]]></script></p>

<p>So, if a vector of dimension <script type="math/tex">(3 \times 1)</script> is multiplied by A, we’ll get a <script type="math/tex">(3 \times 1)</script> vector. Seems nice! It looks like <script type="math/tex">A</script> produces a vector in 3D. But, it’s not the case! if we look carefully, then we can see that, there are only 2 independent column in <script type="math/tex">A</script>!</p>

<script type="math/tex; mode=display">C_3 = C_1 + 2C_2</script>

<p>where <script type="math/tex">C_1, C_2</script> &amp; <script type="math/tex">C_3</script> are the columns of <script type="math/tex">A</script> <br />
So, what this means is, these 3 columns/vectors are <strong>Coplanar</strong>, so taking any linear combination of these vectors would give us a point in this plane itself! So, the Span of the matrix is <script type="math/tex">\mathcal{R}^2</script> and not <script type="math/tex">\mathcal{R}^3</script>! We can also see this by a simple calculation, take any linear combination of columns of <script type="math/tex">A</script>-</p>

<script type="math/tex; mode=display">C = \alpha_1 C_1 + \alpha_2 C_2 + \alpha_3 C_3</script>

<p>Using <script type="math/tex">(5)</script></p>

<script type="math/tex; mode=display">C = \alpha_1 C_1 + \alpha_2 C_2 + \alpha3 (C_1 + 2C_2) \\
= (\alpha_1 + \alpha_3) C_1 + (\alpha_2 + 2 \alpha_3) C_2 \\
C = \beta_1 C_1 + \beta_2 C_2</script>

<p>So, though it looks like <script type="math/tex">A</script> maps a vector to <script type="math/tex">\mathcal{R}^3</script> but, it actually maps it to <script type="math/tex">\mathcal{R}^2</script>. Hence, rank of the matrix is 2!</p>

<p>That being said, let’s try to visualize the transformation this matrix is performing. So, we know that the input space is <script type="math/tex">\mathcal{R}^3</script> and the output space is <script type="math/tex">\mathcal{R}^2</script>, this means that, the transformation in some way, squashes the entire 3D system into 2D system!<br />
So, what does it have to do with eigenvectors &amp; eigenvalues?  Well the number of non-zero Eigenvectors will be equal to Rank of the matrix. This is quite intuitive, because a 2D plane can only have a maximum of 2 Eigenvectors (any 3rd vector can be represented as sum of these 2), and the 3rd Eigenvector would come from the sqashing of the 3D space for which the Eigenvalue will be 0! <br />
So, thing to take away is the number of non-zero Eigenvectors will be equal to the Rank of the Matrix.</p>

<p><strong>Note</strong>: This discussion is valid for any <script type="math/tex">n</script>-dimensional matrix.</p>

<h2 id="so-why-do-they-pop-up-everywhere">So why do they pop up everywhere?</h2>

<p>Well simply put, Eigenvectors make understanding/visualizing Linear transforms easier! We can analyse any Linear transform as stretching or compressing of a vector. Which makes things much easier, we can always decompose a matrix in terms of it’s eigenvectors &amp; eigenvalues.</p>

<script type="math/tex; mode=display">A = V^{-1} \Lambda V</script>

<p>where <script type="math/tex">V</script> is the matrix containing Eigenvectors &amp; <script type="math/tex">\Lambda</script> is a Diagonal matrix with Eigenvalues as the diagonal values. This Factorisation is also called <strong>Eigenvalue decomposition</strong>, which is heavily used in Control Theory &amp; Signal Processing.</p>

<p>Well that’s it! Hope I have explained the concepts clearly. Any feedback will be highly valued.</p>

<h2 id="footnotes">Footnotes</h2>

<ol>
  <li>
    <p><a href="https://www.youtube.com/watch?v=PFDu9oVAE-g">3Blue 1Brown’s video on Eigenvectors &amp; Eigenvalues</a> is a really nice video! The animations were really helpful for me while understanding these concepts.</p>
  </li>
  <li>
    <p><a href="https://setosa.io/ev/eigenvectors-and-eigenvalues/">Setosa.io’s blog on Eigenvectors &amp; Eigenvalues</a> are also nice tool to visualize the same.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=DzqE7tj7eIM">Gilbert Strang’s lecture on Eigenvectors &amp; Eigenvalues</a> give a very theorotical insight into Eigenvectors.</p>
  </li>
</ol>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://backpropagator.github.io/tags/#eigenvectors" class="page__taxonomy-item" rel="tag">Eigenvectors</a><span class="sep">, </span>
    
      
      
      <a href="https://backpropagator.github.io/tags/#linear-algebra" class="page__taxonomy-item" rel="tag">Linear Algebra</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://backpropagator.github.io/posts/eigenvector.md/" class="btn btn--twitter" title="Share on Twitter"><i class="fab fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://backpropagator.github.io/posts/eigenvector.md/" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://backpropagator.github.io/posts/eigenvector.md/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="https://backpropagator.github.io/posts/2019/08/blog-post-4/" class="pagination--pager" title="Useful links for research
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
      

<div class="page__comments">
  
  
    <h4 class="page__comments-title">Leave a Comment</h4>
    <section id="disqus_thread"></section>
  
</div>
    
  </article>

  
  
    <div class="page__related">
      
        <h4 class="page__related-title">You May Also Enjoy</h4>
      
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://backpropagator.github.io/posts/2019/08/blog-post-4/" rel="permalink">Useful links for research
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2019-08-01T00:00:00-07:00">August 01, 2019</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h2 id="books">Books</h2>
<ul>
  <li><a href="http://pgm.stanford.edu/">Probabilistic Graphical Models</a></li>
  <li><a href="http://www.deeplearningbook.org/">Deep learning</a></li>
  <li><a href="https://www.microsoft.com/en-us/research/publication/deep-learning-methods-and-applications/">Deep Learning: Methods and Applications</a></li>
</ul>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://backpropagator.github.io/posts/ml.md/" rel="permalink">Machine Learning Resources
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  25 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2019-07-13T00:00:00-07:00">July 13, 2019</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h2 id="resources">Resources</h2>
<h3 id="table-of-contents">Table of Contents</h3>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://backpropagator.github.io/posts/2019-07-01-competitive-journal-2019.md/" rel="permalink">Competitive programming journal 2019
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  8 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2019-07-01T00:00:00-07:00">July 01, 2019</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h4 id="4-january">4 January</h4>
<ul>
  <li>Participated in <a href="https://codeforces.com/contest/1097">Hello 2019</a>. First 2 problems were easy, though the 2nd one couldn’t pass system tests just because size of one of the arrays was less than what it was expected, again dissappointed. Rank : 5617, 1340-&gt;1292.</li>
</ul>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://backpropagator.github.io/posts/2018-12-31-competitive-journal-2018.md/" rel="permalink">Competitive programming journal 2018
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  2 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-12-31T00:00:00-08:00">December 31, 2018</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h4 id="may-2018">May 2018</h4>
<ul>
  <li><a href="https://piyushtiwary31.wordpress.com/2018/05/">May 2018 Journal</a></li>
</ul>

</p>
    
    
    

  </article>
</div>

        
      </div>
    </div>
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/backpropagator"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://backpropagator.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Piyush Tiwary. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://backpropagator.github.io/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  
  <script type="text/javascript">
  	/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  	var disqus_shortname = 'aquarius31-github-io';

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function() {
  		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  	})();

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function () {
  		var s = document.createElement('script'); s.async = true;
  		s.type = 'text/javascript';
  		s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  		(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  	}());
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>






  </body>
</html>

